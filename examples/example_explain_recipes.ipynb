{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 11:57:38.291942: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-18 11:57:38.291977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-18 11:57:38.293346: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-18 11:57:38.300467: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-18 11:57:39.151313: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Tuple, Union, Callable, Set\n",
    "from enum import Enum\n",
    "from abc import ABC, abstractclassmethod, abstractmethod\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import json\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict  # For word frequency\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/victor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load recipes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>cultural_restriction</th>\n",
       "      <th>calories</th>\n",
       "      <th>allergies</th>\n",
       "      <th>recipeId</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>preparation</th>\n",
       "      <th>carbs</th>\n",
       "      <th>fat</th>\n",
       "      <th>fiber</th>\n",
       "      <th>protein</th>\n",
       "      <th>taste</th>\n",
       "      <th>cooking_style</th>\n",
       "      <th>meal_type</th>\n",
       "      <th>prep_time</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>price</th>\n",
       "      <th>ingredients_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fruit Salad</td>\n",
       "      <td>1. Fruit Salad: 70 calories per portion, 4 por...</td>\n",
       "      <td>vegan</td>\n",
       "      <td>70.0</td>\n",
       "      <td>contains fruits only</td>\n",
       "      <td>food_0</td>\n",
       "      <td>ingredients:\\n- 1 apple\\n- 1 banana\\n- 1 orang...</td>\n",
       "      <td>\\n1. wash and cut all the fruits into bite-siz...</td>\n",
       "      <td>223.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>sweet</td>\n",
       "      <td>mixed</td>\n",
       "      <td>fruit-based</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fruit Salad - Mixed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>apple, banana, orange, grape, strawberry, pine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vegan Omelette</td>\n",
       "      <td>5. Vegan Omelette: 300 calories per portion, 1...</td>\n",
       "      <td>vegan</td>\n",
       "      <td>300.0</td>\n",
       "      <td>contains chickpea flour</td>\n",
       "      <td>food_4</td>\n",
       "      <td>ingredients:\\n- 1 cup chickpea flour\\n- 1 cup ...</td>\n",
       "      <td>\\n1. in a mixing bowl, whisk together the chic...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Savory</td>\n",
       "      <td>sauteed</td>\n",
       "      <td>veggie</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Vegan Omelette - Vegan</td>\n",
       "      <td>2.0</td>\n",
       "      <td>chickpea, flour, water, yeast, turmeric, garli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vegan French Toast</td>\n",
       "      <td>7. Vegan French Toast: 400 calories per portio...</td>\n",
       "      <td>vegan</td>\n",
       "      <td>400.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>food_6</td>\n",
       "      <td>ingredients:\\n- 4 slices of vegan bread\\n- 1 c...</td>\n",
       "      <td>\\n1. in a shallow dish, whisk together the alm...</td>\n",
       "      <td>115.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>11.8</td>\n",
       "      <td>17.0</td>\n",
       "      <td>sweet</td>\n",
       "      <td>sauteed</td>\n",
       "      <td>vegan</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Vegan French Toast: French</td>\n",
       "      <td>1.0</td>\n",
       "      <td>vegan, bread, unsweetened, almond, milk, groun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                title                                           raw_text  \\\n",
       "0         Fruit Salad  1. Fruit Salad: 70 calories per portion, 4 por...   \n",
       "1      Vegan Omelette  5. Vegan Omelette: 300 calories per portion, 1...   \n",
       "2  Vegan French Toast  7. Vegan French Toast: 400 calories per portio...   \n",
       "\n",
       "  cultural_restriction  calories                allergies recipeId  \\\n",
       "0                vegan      70.0     contains fruits only   food_0   \n",
       "1                vegan     300.0  contains chickpea flour   food_4   \n",
       "2                vegan     400.0                      NaN   food_6   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  ingredients:\\n- 1 apple\\n- 1 banana\\n- 1 orang...   \n",
       "1  ingredients:\\n- 1 cup chickpea flour\\n- 1 cup ...   \n",
       "2  ingredients:\\n- 4 slices of vegan bread\\n- 1 c...   \n",
       "\n",
       "                                         preparation  carbs   fat  fiber  \\\n",
       "0  \\n1. wash and cut all the fruits into bite-siz...  223.0   2.0    0.0   \n",
       "1  \\n1. in a mixing bowl, whisk together the chic...  100.0  20.5    6.0   \n",
       "2  \\n1. in a shallow dish, whisk together the alm...  115.0  38.5   11.8   \n",
       "\n",
       "   protein   taste cooking_style    meal_type  prep_time  \\\n",
       "0     15.0   sweet         mixed  fruit-based        NaN   \n",
       "1     15.0  Savory       sauteed       veggie       20.0   \n",
       "2     17.0   sweet       sauteed        vegan       25.0   \n",
       "\n",
       "                      cuisine price  \\\n",
       "0         Fruit Salad - Mixed   2.0   \n",
       "1      Vegan Omelette - Vegan   2.0   \n",
       "2  Vegan French Toast: French   1.0   \n",
       "\n",
       "                                    ingredients_list  \n",
       "0  apple, banana, orange, grape, strawberry, pine...  \n",
       "1  chickpea, flour, water, yeast, turmeric, garli...  \n",
       "2  vegan, bread, unsweetened, almond, milk, groun...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recipes = pd.read_csv(os.path.join(base_path, 'data', 'df_final_7000.csv'), sep='|', index_col=0)\n",
    "df_recipes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing recipes\n",
    "df_recipes.rename(columns={\"allergies\": 'allergens'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan in columns\n",
    "dict_col_nans = {}\n",
    "for col in df_recipes.columns:\n",
    "  dict_col_nans[col] = sum(df_recipes[col].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 0,\n",
       " 'raw_text': 0,\n",
       " 'cultural_restriction': 805,\n",
       " 'calories': 0,\n",
       " 'allergens': 4938,\n",
       " 'recipeId': 0,\n",
       " 'ingredients': 0,\n",
       " 'preparation': 15,\n",
       " 'carbs': 62,\n",
       " 'fat': 180,\n",
       " 'fiber': 735,\n",
       " 'protein': 120,\n",
       " 'taste': 1,\n",
       " 'cooking_style': 779,\n",
       " 'meal_type': 756,\n",
       " 'prep_time': 1709,\n",
       " 'cuisine': 4,\n",
       " 'price': 0,\n",
       " 'ingredients_list': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_col_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan\n",
    "df_recipes['cultural_restriction'].fillna('NotCulturalRestriction', inplace=True)\n",
    "df_recipes['allergens'].fillna('NotAllergenRegistered', inplace=True)\n",
    "df_recipes['taste'].fillna('sweet', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_recipes[\"ingredients\"].isna()\n",
    "df_recipes_clean = df_recipes[~mask].copy()\n",
    "sum(df_recipes_clean[\"ingredients\"].isna())\n",
    "df_recipes_clean[\"title\"] =  df_recipes_clean[\"title\"].apply(lambda x: x.split(\"\\n\")[0].strip())\n",
    "df_recipes_clean.drop_duplicates(subset='title', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stop_words = list(set(stopwords.words('english')))\n",
    "list_measures = [\"bag\", \"bags\", \"liter\", \"liters\", \"bar\", \"bottle\", \"bottles\", \"bowl\", \"bowls\", \"box\", \"boxes\", \"carton\", \"jar\", \"jars\",\n",
    "                 \"cup\", \"cups\", \"drop\", \"glass\", \"piece\", \"roll\", \"rolles\", \"slice\", \"slices\", \"spoon\", \"spoons\", \"spoonful\", \"lbs\",\n",
    "                 \"all-purpose\", \"purpose\", \"diced\", \"sliced\", \"shopped\",\"oil\",\n",
    "                 \"tablespoon\", \"tablespoons\", \"large\", \"teaspoon\", \"teaspoons\", \"tube\", \"chunks\", \"chunk\", \"dice\", \"dices\", \"juice\", \"use\", \"contains\",\n",
    "                 \"contain\"]\n",
    "country_names = ['united','state',  'north', 'america', 'italian', 'mediterranean', 'swiss', 'mexico', 'american', 'middle', 'eastern',\n",
    "                 'asian', 'usa', 'japan', 'indian', 'moroccan', 'cuban', 'russian', 'japanese', 'british', 'italy',  'greece', 'france',\n",
    "                 'vietnam', 'turkish', 'holland', 'lebanese', 'belgian', 'india', 'indonesian',  'chilean', 'syrian', 'venezuelan', 'ireland',\n",
    "                 'swedish', 'filipino', 'polish', 'singaporean', 'israeli', 'brazilian', 'sri', 'lanka', 'jamaican',  'finnish', 'karelian',\n",
    "                 'afghan',   'nigerian', 'egyptian',     'haitian', 'iraqi', 'maltese', 'algerian', 'canadian',  'ethiopian', 'iranian', 'malaysian',\n",
    "                 'arabic', 'norwegian', 'brazil', 'belgium', 'russia', 'egypt', 'pakistan', 'dutch', 'african', 'malaysia', 'spain', 'korea', 'lebanon',\n",
    "                 'tunisian', 'scotland', 'china', 'iran', 'hungarian',  'monterrey', 'latin', 'southern', 'persian', 'argentina', 'albanian', 'scottish',\n",
    "                 'california', 'israel', 'east',  'spanish', 'irish',  'scandinavia', 'canada',  'southeast', 'asia', 'mongolian',\n",
    "]\n",
    "additional_filter = ['optional', 'ripe', 'cooked', 'nutritional', 'chopped', 'cooking', 'powdered', 'serving', 'mixed', 'block','firm',\n",
    "'drained', 'pressed', 'rinsed', 'melted', 'english', 'type', 'choice', 'pound', 'crumbled','small', 'vital', 'premade', 'scramble', 'tbsp', 'finely',\n",
    "'sized', 'medium', 'ounce', 'smoke', 'peeled', 'grated', 'pre', 'made', 'based', 'granulated', 'cold', 'cubed', 'add', 'in', 'recipe', 'breakfast',\n",
    "'mashed', 'free', 'nib', 'active', 'dry', 'softened', 'packed', 'kernel', 'juiced', 'sheet', 'julienned',   'day','old', 'meal', 'tsp', 'stick',\n",
    "'star', 'inch', 'removed', 'store',  'bought', 'homemade', 'extra',  'least', 'hour', 'refrigerated', 'pocket', 'warm', 'etc', 'shred', 'thick',\n",
    "'round', 'package', 'shaving', 'plus', 'simple', 'choose','favorite', 'preferred', 'scrambled', 'additional', 'cut', 'strip','paper', 'kind','prefer',\n",
    "'can', 'overnight','frying','squash', 'apologize', 'language', 'model', 'provide','specific','trained','however','offer','general', 'substituted',\n",
    "'desired','piece', 'matcha', 'bar', 'added', 'pan', 'crushed',  'calorie','per','portion', 'mixture', 'soft', 'roughly',  'low', 'nutrition',\n",
    "'colder', 'thin', 'thinning','drizzle', 'suggested', 'energy', 'bite', 'mini', 'desiccated', 'friendly', 'half',  'serve', 'approximately', 'ingredient',\n",
    "'preparation', 'step',  'listed', 'please', 'note', 'count', 'may', 'vary', 'depending', 'size', 'brand', 'used', 'frosting', 'instant','icing', 'frothing',\n",
    "'slightly', 'stale', 'dusting', 'approx', 'adjust', 'according', 'whipped', 'heavy', 'crumb', 'includes', 'following', 'total', 'sorry', 'access',\n",
    "'database', 'tbd', 'dressing', 'quantity', 'best', 'calculate', 'unfortunately', 'ability', 'information', 'like', 'give', 'online', 'tool', 'apps',\n",
    "'text','directly', 'certainly', 'help', 'varies', 'rough', 'estimate', 'around', 'provides',  'depend', 'amount', 'hulled', 'estimated', 'button', 'fact',\n",
    "'need', 'make', 'omitted', 'along', 'still', 'real', 'time', 'unable', 'individual', 'exact', 'without', 'knowing',  'consistency', 'beaten', 'culture',\n",
    "'estimating', 'approximate', 'uncooked', 'average', 'range', 'allergic', 'warning',  'kcals', 'unknown', 'title', 'bay',  'street', 'frank', 'redhot',\n",
    "'cuisine', 'grilled', 'solid', 'start', 'using', 'grater', 'food', 'processor', 'place', 'clean', 'kitchen', 'towel', 'squeeze', 'excess', 'mixing',\n",
    "'combine', 'mix', 'well', 'evenly', 'combined', 'heat', 'skillet', 'take', 'form', 'compact', 'heated', 'cook', 'minute', 'side', 'golden', 'remove',\n",
    "'lined', 'plate', 'absorb', 'repeat', 'process', 'remaining', 'adding', 'divided', 'deep', 'undrained', 'young', 'trimmed', 'thread', 'coloring',\n",
    "'blanched', 'fried', 'instruction', 'cleaned', 'slider', 'steamed','jumbo', 'full', 'stir', 'fry', 'snap', 'textured', 'bit', 'snow', 'check',  'shape',\n",
    "'reduction', 'classic', 'sub', 'string', 'metal', 'capability', 'keep', 'mind', 'basic', 'substitute',  'part', 'prepared', 'separately', 'ripened',\n",
    "'label', 'ring', 'shortening', 'boat', 'ing', 'suitable',  'rehydrated', 'sparkling', 'fermented', 'style',  'ensure', 'required', 'regular', 'necessary',\n",
    "'allergy', 'giant', 'gigantes', 'diagonally', 'split', 'blended','mild', 'sure', 'ensure', 'required', 'regular', 'necessary', 'allergy','dish','one',\n",
    "'toothpick', 'generate', 'roasting', 'hard', 'yolk', 'everything', 'coarse', 'trail', 'ready', 'kilogram', 'combination', 'pit', 'flesh', 'mash', 'fork',\n",
    "'cover', 'plastic', 'pressing', 'onto', 'surface', 'prevent', 'browning', 'refrigerate', 'allow', 'meld', 'let','move', 'depends', 'generally', 'great',\n",
    "'top', 'marinade', 'rib', 'segmented', 'northern', 'flat', 'people', 'regional', 'influence', 'popular', 'delicious', 'lover', 'preserved', 'end', 'similar',\n",
    "'see', 'eyed', 'fine', 'century', 'semi', 'johnny', 'recommended', 'calculator', 'determine', 'value',  'traditional', 'typically', 'available', 'kcal',\n",
    "'able', 'achieve', 'room', 'temperature', 'circular', 'bubble', 'bottom', 'carefully', 'flip', 'another', 'stack', 'follows', 'precooked', 'tongue',\n",
    "'mentioned', 'replace', 'fermentation', 'center', 'tilt', 'motion', 'appear', 'edge','lift', 'cool', 'bolillos', 'quick', 'easy', 'exception', 'try',\n",
    "'parfait', 'root', 'loose', 'suggestion', 'thickness', 'fully', 'also', 'known', 'loosely', 'chop', 'could',  'quality', 'slicing', 'increase', 'assist',\n",
    "'providing', 'prepare', 'grade', 'estimation', 'would', 'data', 'caloric', 'quarter', 'provided', 'list', 'including', 'precise', 'associated','important',\n",
    "'content', 'number', 'yield', 'know', 'considered', 'segment', 'miniature', 'preheat', 'oven', 'line', 'cutter', 'resembles', 'overmix', 'turn',  'wire',\n",
    "'rack', 'follow','continue', 'imitation', 'spider', 'allergen', 'caution', 'sensitivity', 'account', 'dietary', 'restriction', 'always', 'adapt',\n",
    "'accordingly', 'included', 'different', 'none', 'confirm', 'safe', 'potential', 'trigger', 'cause', 'possible', 'intolerance', 'texture', 'sealing', 'exclude',\n",
    "'dinner', 'club', 'saturated', 'cholesterol', 'carbohydrate', 'fiber', 'factor','idea', 'covered', 'course', 'reduce', 'immersion', 'blender', 'source',\n",
    "'healthier', 'measurement', 'bomba', 'avoid', 'product', 'present', 'easily', 'adapted', 'accommodate', 'find', 'sure', 'substitution', 'common', 'sharp',\n",
    "'alternative', 'non', 'dash', 'purpose', 'difficulty', 'intermediate', 'garden', 'intolerant','avoiding', 'skip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_stop_words = list_stop_words + list_measures + additional_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/victor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/victor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_nltk(text:str, stop_words:List[str]=None, lema=False, steam=False):\n",
    "  # 1 lowercase\n",
    "  new_text = text.lower()\n",
    "  new_text = re.sub(r\"(ingredient|ingredients).*:\", \" \", new_text)\n",
    "  # remove numbers\n",
    "  new_text = re.sub(r\"\\d+\", \" \", new_text)\n",
    "  # 2 Removing puntuation\n",
    "  new_text = re.sub(r'[^A-Za-z0-9 ]+', ' ', new_text)\n",
    "  # 3 Tokenization\n",
    "  words = word_tokenize(new_text)\n",
    "  # 4 Stop words filtering\n",
    "  if stop_words:\n",
    "    filtered_words = [w for w in words if not w in stop_words]\n",
    "  else:\n",
    "    filtered_words = words\n",
    "  # Filtering short words\n",
    "  filtered_words = list(filter(lambda x: len(x)>2, filtered_words))\n",
    "  if not lema and not steam:\n",
    "    return filtered_words\n",
    "  elif not lema and steam:\n",
    "    # 5 Stemming\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(w) for w in filtered_words]\n",
    "    return stemmed\n",
    "  else:\n",
    "    # 6 lematization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemas = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    return lemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chickpea flour water yeast turmeric garlic powder onion powder salt pepper taste bell pepper onion tomato fresh cilantro'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_words = text_preprocessing_nltk(df_recipes[\"ingredients\"][1], stop_words=total_stop_words, lema=True)\n",
    "recipe_text = \" \".join(list_of_words)\n",
    "recipe_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=1.2)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_nltk = partial(text_preprocessing_nltk, stop_words=total_stop_words, lema=True)\n",
    "recipes_corpus = df_recipes_clean[\"ingredients\"].apply(lambda x: \" \".join(processing_nltk(x)))\n",
    "list_sentences = recipes_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(list_sentences: list[str]):\n",
    "  for sent in list_sentences:\n",
    "    tokens = gensim.utils.simple_preprocess(sent)\n",
    "    yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(generate_corpus(list_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6937"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2023"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in list_sentences:\n",
    "    for i in sent.split(\" \"):\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pepper',\n",
       " 'salt',\n",
       " 'fresh',\n",
       " 'taste',\n",
       " 'onion',\n",
       " 'garlic',\n",
       " 'olive',\n",
       " 'minced',\n",
       " 'clove',\n",
       " 'cheese']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = list(word_freq.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(916970, 39)\n",
      "(229243, 39)\n",
      "(286554, 39)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(base_path, 'data','train_data.csv'))\n",
    "print(train.shape)\n",
    "val = pd.read_csv(os.path.join(base_path, 'data', \"val_data.csv\"))\n",
    "print(val.shape)\n",
    "test = pd.read_csv(os.path.join(base_path, 'data', 'test_data.csv'))\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (StringLookup,\n",
    "                                     TextVectorization,\n",
    "                                     Embedding,\n",
    "                                     Normalization,\n",
    "                                     Concatenate,\n",
    "                                     Flatten,\n",
    "                                     Dropout,\n",
    "                                     GlobalAveragePooling1D)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix with the embeddings\n",
    "def get_embeddings_recipe_id(recipe_id: str, embedding_dict: Dict[str, np.array]):\n",
    "  return embedding_dict[recipe_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate features dictionaries\n",
    "def generate_features(df, feature_list, numeric_features):\n",
    "  dict_features = {}\n",
    "  for feature in feature_list:\n",
    "    #print(f\"feature: {feature}\")\n",
    "    if feature in numeric_features:\n",
    "      #print(f\"feature numeric: {feature}\")\n",
    "      dict_features[feature] = tf.convert_to_tensor(df[feature].values, tf.float32, name=f\"{feature}\")\n",
    "    else:\n",
    "      dict_features[feature] = tf.convert_to_tensor(df[feature].values, tf.string, name=f\"{feature}\")\n",
    "  return dict_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity layer\n",
    "class CosineSimilarity(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(CosineSimilarity, self).__init__()\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    pass\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x, y = inputs\n",
    "    dot = tf.reduce_sum(x*y, axis=0)\n",
    "    #print(f\"dot: {dot}\")\n",
    "    norm_x = tf.norm(x, axis=1)\n",
    "    norm_y = tf.norm(y, axis=1)\n",
    "    #print(f\"norm x: {norm_x}\")\n",
    "    #print(f\"norm y: {norm_y}\")\n",
    "    sim = tf.math.divide_no_nan(dot, norm_x*norm_y)\n",
    "    return tf.expand_dims(sim, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs(feature_names: List[str],\n",
    "                        numeric_features: List[str],\n",
    "                        shapes_dict: Dict[str, Tuple] = None):\n",
    "    inputs = {}\n",
    "    for feature_name in feature_names:\n",
    "      shape = ()\n",
    "      if shapes_dict is not None and feature_name in shapes_dict.keys():\n",
    "        shape = shapes_dict[feature_name]\n",
    "      if feature_name in numeric_features:\n",
    "          inputs[feature_name] = tf.keras.layers.Input(\n",
    "              name=feature_name, shape=shape, dtype=tf.float32\n",
    "          )\n",
    "      else:\n",
    "          inputs[feature_name] = tf.keras.layers.Input(\n",
    "              name=feature_name, shape=shape, dtype=tf.string\n",
    "          )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.metrics.confusion_metrics import activations\n",
    "from keras.src.utils.feature_space import Feature\n",
    "from tensorflow.python.eager import context\n",
    "def encode_inputs(inputs, categorical_dict, user_features, recipes_features=[], context_features=[]):\n",
    "    encoded_features = []\n",
    "    user_features_list = []\n",
    "    recipes_features_list = []\n",
    "    context_features_list = []\n",
    "    for feature_name in inputs:\n",
    "      # check if the input is in the list\n",
    "      if feature_name in user_features_list or feature_name in recipes_features or context_features in context_features:\n",
    "        print(f\"Processing feature: {feature_name}...\")\n",
    "        if feature_name in categorical_dict:\n",
    "            embedding_type = categorical_dict.get(feature_name)\n",
    "            if embedding_type is not None:\n",
    "                if embedding_type[\"embedding_type\"] == \"text\":\n",
    "                    # Vocabulary size and number of words in a sequence.\n",
    "                    vocab_size = 10000\n",
    "                    sequence_length = embedding_type.get(\"max_seq_len\", 10)\n",
    "                    # Use the text vectorization layer to normalize, split, and map strings to\n",
    "                    # integers. Note that the layer uses the custom standardization defined above.\n",
    "                    # Set maximum_sequence length as all samples are not of the same length.\n",
    "                    text_dataset = \\\n",
    "                    tf.data.Dataset.from_tensor_slices(\n",
    "                        np.expand_dims(train[feature_name].to_numpy().astype(str), -1)\n",
    "                        )\n",
    "                    vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "                        standardize=custom_standardization,\n",
    "                        max_tokens=vocab_size,\n",
    "                        output_mode='int',\n",
    "                        output_sequence_length=10\n",
    "                    )\n",
    "                    vectorize_layer.adapt(text_dataset.batch(64))\n",
    "                    vectorized_text = vectorize_layer(inputs[feature_name])\n",
    "                    embedding_text = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                output_dim=100,\n",
    "                                                mask_zero=True)(vectorized_text)\n",
    "                    flatten_layer = tf.keras.layers.Flatten()\n",
    "                    encoded_feature = flatten_layer(embedding_text)\n",
    "                else:\n",
    "                    vocabulary = categorical_dict[feature_name][\"vocabulary\"]\n",
    "                    # Create a lookup to convert string values to an integer indices.\n",
    "                    # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "                    # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "                    if embedding_type[\"embedding_type\"] == \"sparse\":\n",
    "                        use_embedding = False\n",
    "                    else:\n",
    "                        use_embedding = True\n",
    "                    lookup = StringLookup(\n",
    "                    vocabulary=vocabulary,\n",
    "                    mask_token=None,\n",
    "                    num_oov_indices=0,\n",
    "                    output_mode=\"int\" if use_embedding else \"binary\",\n",
    "                    )\n",
    "                    if use_embedding:\n",
    "                        # Convert the string input values into integer indices.\n",
    "                        encoded_feature = lookup(inputs[feature_name])\n",
    "                        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "                        # Create an embedding layer with the specified dimensions.\n",
    "                        embedding = tf.keras.layers.Embedding(\n",
    "                            input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "                        )\n",
    "                        # Convert the index values to embedding representations.\n",
    "                        encoded_feature = embedding(encoded_feature)\n",
    "                    else:\n",
    "                        # Convert the string input values into a one hot encoding.\n",
    "                        encoded_feature = lookup(tf.expand_dims(inputs[feature_name], -1))\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            data = tf.data.Dataset.from_tensor_slices(\n",
    "                np.expand_dims(train[feature_name].to_numpy().astype(np.float32), -1))\n",
    "            normalization_layer = Normalization(axis=-1, input_dim=1)\n",
    "            normalization_layer.adapt(data, steps=40)\n",
    "            encoded_feature = normalization_layer(tf.expand_dims(inputs[feature_name], -1))\n",
    "            #encoded_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "        # create submodels\n",
    "        if feature_name in user_features:\n",
    "          user_features_list.append(encoded_feature)\n",
    "        elif len(recipes_features) > 0 and feature_name in recipes_features:\n",
    "          recipes_features_list.append(encoded_feature)\n",
    "        elif len(context_features) > 0:\n",
    "          context_features_list.append(encoded_feature)\n",
    "        else:\n",
    "          pass\n",
    "\n",
    "    # create submodels\n",
    "    # user sub model\n",
    "    user_concat = tf.keras.layers.concatenate(user_features_list)\n",
    "    user_concat = tf.keras.layers.Dense(units=100, activation=\"linear\",\n",
    "                                 kernel_regularizer=\"l2\",\n",
    "                               name=\"user_embedding\")(user_concat)\n",
    "    # recipes sub model\n",
    "    if len(recipes_features) > 0:\n",
    "      recipes_concat = tf.keras.layers.concatenate(recipes_features_list)\n",
    "      recipes_concat = tf.keras.layers.Dense(units=100, activation=\"linear\",\n",
    "                                  kernel_regularizer=\"l2\",\n",
    "                                    name=\"recipes_embedding\")(recipes_concat)\n",
    "    else:\n",
    "      recipes_concat = None\n",
    "\n",
    "    # context sub model\n",
    "    if len(context_features) > 0:\n",
    "      context_concat = tf.keras.layers.concatenate(context_features_list)\n",
    "      context_concat = tf.keras.layers.Dense(units=32, activation=\"linear\",\n",
    "                                  kernel_regularizer=\"l2\",\n",
    "                                    name=\"context_embedding\")(context_concat)\n",
    "    else:\n",
    "      context_concat = None\n",
    "\n",
    "    #all_features = layers.concatenate([user_concat, recipes_concat, context_concat])\n",
    "    #all_features = Concatenate()(encoded_features)\n",
    "    return user_concat, recipes_concat, context_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(model_inputs, feature_list,\n",
    "                    categorical_dict,\n",
    "                    numerical_features,\n",
    "                    final_layer_size=100,\n",
    "                    layer_name = 'concat_layer',\n",
    "                    embedding_list = None,\n",
    "                    defined_input_shape=False):\n",
    "  encoded_features_list = []\n",
    "  for feature_name in feature_list:\n",
    "    # check if the input is in the list\n",
    "    print(f\"Processing feature: {feature_name}...\")\n",
    "    if feature_name in categorical_dict:\n",
    "      embedding_type = categorical_dict.get(feature_name)\n",
    "      if embedding_type is not None:\n",
    "        if embedding_type[\"embedding_type\"] == \"text\":\n",
    "          # Vocabulary size and number of words in a sequence.\n",
    "          vocab_size = 10000\n",
    "          sequence_length = embedding_type.get(\"max_seq_len\", 10)\n",
    "          # Use the text vectorization layer to normalize, split, and map strings to\n",
    "          # integers. Note that the layer uses the custom standardization defined above.\n",
    "          # Set maximum_sequence length as all samples are not of the same length.\n",
    "          text_dataset = \\\n",
    "          tf.data.Dataset.from_tensor_slices(\n",
    "              np.expand_dims(train[feature_name].to_numpy().astype(str), -1)\n",
    "              )\n",
    "          vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "              standardize=custom_standardization,\n",
    "              max_tokens=vocab_size,\n",
    "              output_mode='int',\n",
    "              output_sequence_length=10\n",
    "          )\n",
    "          vectorize_layer.adapt(text_dataset.batch(64))\n",
    "          vectorized_text = vectorize_layer(model_inputs[feature_name])\n",
    "          embedding_text = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                      output_dim=100,\n",
    "                                      mask_zero=True)(vectorized_text)\n",
    "          flatten_layer = tf.keras.layers.Flatten()\n",
    "          encoded_feature = flatten_layer(embedding_text)\n",
    "        else:\n",
    "          vocabulary = categorical_dict[feature_name][\"vocabulary\"]\n",
    "          # Create a lookup to convert string values to an integer indices.\n",
    "          # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "          # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "          if embedding_type[\"embedding_type\"] == \"sparse\":\n",
    "              use_embedding = False\n",
    "          else:\n",
    "              use_embedding = True\n",
    "          lookup = StringLookup(\n",
    "          vocabulary=vocabulary,\n",
    "          mask_token=None,\n",
    "          num_oov_indices=0,\n",
    "          output_mode=\"int\" if use_embedding else \"binary\",\n",
    "          )\n",
    "          if use_embedding:\n",
    "              # Convert the string input values into integer indices.\n",
    "              encoded_feature = lookup(model_inputs[feature_name])\n",
    "              embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "              # Create an embedding layer with the specified dimensions.\n",
    "              embedding = tf.keras.layers.Embedding(\n",
    "                  input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "              )\n",
    "              # Convert the index values to embedding representations.\n",
    "              encoded_feature = embedding(encoded_feature)\n",
    "              # Flatten the embedding layer to a 1D tensor.\n",
    "              if defined_input_shape:\n",
    "                encoded_feature = tf.keras.layers.Flatten()(encoded_feature)\n",
    "          else:\n",
    "              # Convert the string input values into a one hot encoding.\n",
    "              if defined_input_shape:\n",
    "                encoded_feature = lookup(model_inputs[feature_name])\n",
    "              else:\n",
    "                encoded_feature = lookup(tf.expand_dims(model_inputs[feature_name], -1))\n",
    "    else:\n",
    "      # Use the numerical features as-is.\n",
    "      # data = tf.data.Dataset.from_tensor_slices(\n",
    "      #     np.expand_dims(train[feature_name].to_numpy().astype(np.float32), -1))\n",
    "      # normalization_layer = Normalization(input_shape=(1,))\n",
    "      # normalization_layer.adapt(data, steps=40)\n",
    "      # encoded_feature = normalization_layer(tf.expand_dims(model_inputs[feature_name], -1))\n",
    "      if embedding_list is not None:\n",
    "        if feature_name in embedding_list:\n",
    "          encoded_feature = model_inputs[feature_name]\n",
    "        else:\n",
    "          if defined_input_shape:\n",
    "            encoded_feature = model_inputs[feature_name]\n",
    "          else:\n",
    "            encoded_feature = tf.expand_dims(model_inputs[feature_name], -1)\n",
    "      else:\n",
    "        if defined_input_shape:\n",
    "          encoded_feature = model_inputs[feature_name]\n",
    "        else:\n",
    "          encoded_feature = tf.expand_dims(model_inputs[feature_name], -1)\n",
    "    encoded_features_list.append(encoded_feature)\n",
    "  all_features = tf.keras.layers.concatenate(encoded_features_list)\n",
    "  all_features = tf.keras.layers.Dense(units=final_layer_size,\n",
    "                              activation=\"linear\",\n",
    "                              kernel_regularizer=\"l2\",\n",
    "                              name=layer_name)(all_features)\n",
    "  return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(base_path, 'data', 'full_recipes_sentence_embedding_USE_v2.npz')\n",
    "dict_embeddings_cbow = dict(np.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = train[\"foodId\"].isin(list(dict_embeddings_cbow.keys()))\n",
    "filtered_train =train.loc[mask_train, :]\n",
    "mask_val = val[\"foodId\"].isin(list(dict_embeddings_cbow.keys()))\n",
    "filtered_val = val.loc[mask_val, :]\n",
    "mask_test = test[\"foodId\"].isin(list(dict_embeddings_cbow.keys()))\n",
    "filtered_test = test.loc[mask_test, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_list = ['nutrition_goal', 'clinical_gender', 'age_range',\n",
    "                      'life_style', 'weight', 'height',\n",
    "                      'projected_daily_calories', 'current_daily_calories',\n",
    "                      'cultural_factor', 'allergy',\n",
    "                      'current_working_status', 'marital_status', 'ethnicity',\n",
    "                      'BMI', 'next_BMI']\n",
    "#food_features = ['recipe_name', 'recipe_raw_text', 'meal_type_y',\n",
    "#      'food_cultural_class', 'calories', 'allergens', 'recipeId']\n",
    "food_features = ['food_cultural_class', 'calories', 'allergens', 'taste', 'price', 'recipeId']\n",
    "context_features  = ['day_number', 'meal_type_y','time_of_meal_consumption', 'place_of_meal_consumption', 'social_situation_of_meal_consumption']\n",
    "label_list = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_supervision(vector_row: np.array):\n",
    "  index = np.argmax(vector_row)\n",
    "  if index < 2:\n",
    "    return 0.0\n",
    "  else:\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3356214/879706689.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_train.loc[:, \"label\"] = filtered_train[['label__Never', 'label__Avoidable', 'label__Neutral',\n",
      "/tmp/ipykernel_3356214/879706689.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_val.loc[:, \"label\"] = filtered_val[['label__Never', 'label__Avoidable', 'label__Neutral',\n",
      "/tmp/ipykernel_3356214/879706689.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_test.loc[:, \"label\"] = filtered_test[['label__Never', 'label__Avoidable', 'label__Neutral',\n"
     ]
    }
   ],
   "source": [
    "filtered_train.loc[:, \"label\"] = filtered_train[['label__Never', 'label__Avoidable', 'label__Neutral',\n",
    "                'label__Favorable', 'label__Appreciated']].apply(lambda row: generate_supervision(row.to_numpy()), axis=1)\n",
    "filtered_val.loc[:, \"label\"] = filtered_val[['label__Never', 'label__Avoidable', 'label__Neutral',\n",
    "                'label__Favorable', 'label__Appreciated']].apply(lambda row: generate_supervision(row.to_numpy()), axis=1)\n",
    "filtered_test.loc[:, \"label\"] = filtered_test[['label__Never', 'label__Avoidable', 'label__Neutral',\n",
    "                'label__Favorable', 'label__Appreciated']].apply(lambda row: generate_supervision(row.to_numpy()), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join recipes and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train = filtered_train[[\n",
    "    'day_number', 'meal_type_x', 'userId', 'foodId',\n",
    "    'time_of_meal_consumption', 'place_of_meal_consumption',\n",
    "    'social_situation_of_meal_consumption', 'nutrition_goal',\n",
    "    'clinical_gender', 'age_range', 'life_style', 'weight', 'height',\n",
    "       'projected_daily_calories', 'current_daily_calories',\n",
    "       'country_of_origin', 'living_country', 'current_location',\n",
    "       'cultural_factor', 'probabilities', 'allergy', 'Multi-allergy',\n",
    "       'current_working_status', 'marital_status', 'ethnicity', 'BMI',\n",
    "       'next_BMI', 'recipe_name', 'recipe_raw_text', 'meal_type_y',\n",
    "       'food_cultural_class', 'recipeId',\n",
    "       'label__Appreciated', 'label__Avoidable', 'label__Favorable',\n",
    "       'label__Neutral', 'label__Never', 'label'\n",
    "]]\n",
    "filtered_val = filtered_val[[\n",
    "    'day_number', 'meal_type_x', 'userId', 'foodId',\n",
    "    'time_of_meal_consumption', 'place_of_meal_consumption',\n",
    "    'social_situation_of_meal_consumption', 'nutrition_goal',\n",
    "    'clinical_gender', 'age_range', 'life_style', 'weight', 'height',\n",
    "       'projected_daily_calories', 'current_daily_calories',\n",
    "       'country_of_origin', 'living_country', 'current_location',\n",
    "       'cultural_factor', 'probabilities', 'allergy', 'Multi-allergy',\n",
    "       'current_working_status', 'marital_status', 'ethnicity', 'BMI',\n",
    "       'next_BMI', 'recipe_name', 'recipe_raw_text', 'meal_type_y',\n",
    "       'food_cultural_class', 'recipeId',\n",
    "       'label__Appreciated', 'label__Avoidable', 'label__Favorable',\n",
    "       'label__Neutral', 'label__Never', 'label'\n",
    "]]\n",
    "filtered_test = filtered_test[[\n",
    "    'day_number', 'meal_type_x', 'userId', 'foodId',\n",
    "    'time_of_meal_consumption', 'place_of_meal_consumption',\n",
    "    'social_situation_of_meal_consumption', 'nutrition_goal',\n",
    "    'clinical_gender', 'age_range', 'life_style', 'weight', 'height',\n",
    "       'projected_daily_calories', 'current_daily_calories',\n",
    "       'country_of_origin', 'living_country', 'current_location',\n",
    "       'cultural_factor', 'probabilities', 'allergy', 'Multi-allergy',\n",
    "       'current_working_status', 'marital_status', 'ethnicity', 'BMI',\n",
    "       'next_BMI', 'recipe_name', 'recipe_raw_text', 'meal_type_y',\n",
    "       'food_cultural_class', 'recipeId',\n",
    "       'label__Appreciated', 'label__Avoidable', 'label__Favorable',\n",
    "       'label__Neutral', 'label__Never', 'label'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_join = pd.merge(filtered_train,\n",
    "                               df_recipes_clean[['calories', 'allergens', 'taste', 'price', 'recipeId']],\n",
    "                               on='recipeId')\n",
    "filtered_val_join = pd.merge(filtered_val,\n",
    "                               df_recipes_clean[['calories', 'allergens', 'taste', 'price', 'recipeId']],\n",
    "                               on='recipeId')\n",
    "filtered_test_join = pd.merge(filtered_test,\n",
    "                               df_recipes_clean[['calories', 'allergens', 'taste', 'price', 'recipeId']],\n",
    "                               on='recipeId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out users for study evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique users\n",
    "unique_user_id = np.unique(filtered_train_join['userId'])\n",
    "np.random.seed(0)\n",
    "selected_test_user_id = np.random.choice(unique_user_id, size=50)\n",
    "user_based_filtered_train =  filtered_train_join.query(\"userId not in @selected_test_user_id\")\n",
    "val_users_id = selected_test_user_id[:25]\n",
    "test_users_id = selected_test_user_id[25:]\n",
    "user_based_filtered_val =  filtered_val_join.query(\"userId not in @test_users_id\")\n",
    "user_based_filtered_test = filtered_test_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['nutrition_goal', 'clinical_gender', 'age_range', 'life_style',\n",
      "       'weight', 'height', 'projected_daily_calories',\n",
      "       'current_daily_calories', 'cultural_factor', 'allergy',\n",
      "       'current_working_status', 'marital_status', 'ethnicity', 'BMI',\n",
      "       'next_BMI', 'day_number', 'meal_type_y', 'time_of_meal_consumption',\n",
      "       'place_of_meal_consumption', 'social_situation_of_meal_consumption',\n",
      "       'food_cultural_class', 'calories', 'allergens', 'taste', 'price',\n",
      "       'recipeId', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "total_columns = user_features_list+context_features+food_features+label_list\n",
    "preprossed_df = user_based_filtered_train[total_columns]\n",
    "preprossed_df_val = user_based_filtered_val[total_columns]\n",
    "preprossed_df_test = user_based_filtered_test[total_columns]\n",
    "print(preprossed_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cbow embeddings\n",
    "# get_recipe_emb = partial(get_embeddings_recipe_id, embedding_dict=dict_embeddings_cbow)\n",
    "# skipgram embeddings\n",
    "get_recipe_emb = partial(get_embeddings_recipe_id, embedding_dict=dict_embeddings_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3356214/2342519484.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  preprossed_df.loc[:, \"embeddings\"] = preprossed_df[\"recipeId\"].apply(lambda x: get_recipe_emb(x))\n",
      "/tmp/ipykernel_3356214/2342519484.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  preprossed_df_val.loc[:, \"embeddings\"] = preprossed_df_val[\"recipeId\"].apply(lambda x: get_recipe_emb(x))\n",
      "/tmp/ipykernel_3356214/2342519484.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  preprossed_df_test.loc[:, \"embeddings\"] = preprossed_df_test[\"recipeId\"].apply(lambda x: get_recipe_emb(x))\n"
     ]
    }
   ],
   "source": [
    "# Set embeddings \n",
    "preprossed_df.loc[:, \"embeddings\"] = preprossed_df[\"recipeId\"].apply(lambda x: get_recipe_emb(x))\n",
    "preprossed_df_val.loc[:, \"embeddings\"] = preprossed_df_val[\"recipeId\"].apply(lambda x: get_recipe_emb(x))\n",
    "preprossed_df_test.loc[:, \"embeddings\"] = preprossed_df_test[\"recipeId\"].apply(lambda x: get_recipe_emb(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(base_path, 'models', \n",
    "                    \"training_model_0_use_full_inputs_user_food_context_input_shape.tf-20240403T081741Z-001/training_model_0_use_full_inputs_user_food_context_input_shape.tf\")\n",
    "model_context_m1 = tf.keras.models.load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food_cultural_class', 'calories', 'allergens', 'taste', 'price', 'recipeId']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join_dataset = filtered_train_join\n",
    "food_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES_DICT = {\n",
    "'taste': {\"embedding_type\": \"sparse\", \"vocabulary\":\n",
    "    list(df_join_dataset['taste'].unique())},\n",
    "'allergens': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "    list(df_join_dataset['allergens'].unique())},\n",
    "'meal_type_y': {\"embedding_type\": \"sparse\", \"vocabulary\":\n",
    "    list(df_join_dataset['meal_type_y'].unique())},\n",
    " 'userId': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['userId'].unique())},\n",
    "  'recipeId': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['recipeId'].unique())},\n",
    " 'foodId': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['foodId'].unique())},\n",
    " 'place_of_meal_consumption': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['place_of_meal_consumption'].unique())},\n",
    " 'social_situation_of_meal_consumption': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "    list(df_join_dataset['social_situation_of_meal_consumption'].unique())},\n",
    " 'nutrition_goal': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['nutrition_goal'].unique())},\n",
    " 'clinical_gender': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['clinical_gender'].unique())},\n",
    " 'age_range': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['age_range'].unique())},\n",
    " 'life_style': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['life_style'].unique())},\n",
    " 'country_of_origin': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['country_of_origin'].unique())},\n",
    " 'living_country': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['living_country'].unique())},\n",
    " 'cultural_factor': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['cultural_factor'].unique())},\n",
    " 'probabilities': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['probabilities'].unique())},\n",
    " 'allergy': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['allergy'].unique())},\n",
    " 'current_working_status': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['current_working_status'].unique())},\n",
    " 'marital_status': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['marital_status'].unique())},\n",
    " 'ethnicity': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['ethnicity'].unique())},\n",
    " 'BMI': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['BMI'].unique())},\n",
    " 'next_BMI': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['next_BMI'].unique())},\n",
    " 'recipe_name': {\"embedding_type\": \"text\", \"vocabulary\":\n",
    "     list(df_join_dataset['recipe_name'].unique()),\n",
    "     \"max_seq_len\": 5},\n",
    " 'recipe_raw_text': {\"embedding_type\": \"text\", \"vocabulary\":\n",
    "     list(df_join_dataset['recipe_raw_text'].unique()),\n",
    "     \"max_seq_len\": 10},\n",
    " 'food_cultural_class': {\"embedding_type\": \"dense\", \"vocabulary\":\n",
    "     list(df_join_dataset['food_cultural_class'].unique())}\n",
    "#  'allergens': {\"embedding_type\": \"text\", \"vocabulary\":\n",
    "#      list(df_join_dataset['allergens'].unique()),\n",
    "#      \"max_seq_len\": 10}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['weight',\n",
    "                    'height',\n",
    "                    'projected_daily_calories',\n",
    "                    'current_daily_calories',\n",
    "                    'day_number',\n",
    "                    'time_of_meal_consumption',\n",
    "                    'calories', 'embeddings',\n",
    "                    'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(594380, 512)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(preprossed_df[\"embeddings\"].tolist()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='BMI'), name='BMI', description=\"created by layer 'BMI'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='age_range'), name='age_range', description=\"created by layer 'age_range'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='allergens'), name='allergens', description=\"created by layer 'allergens'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='allergy'), name='allergy', description=\"created by layer 'allergy'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='calories'), name='calories', description=\"created by layer 'calories'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='clinical_gender'), name='clinical_gender', description=\"created by layer 'clinical_gender'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='cultural_factor'), name='cultural_factor', description=\"created by layer 'cultural_factor'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='current_daily_calories'), name='current_daily_calories', description=\"created by layer 'current_daily_calories'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='current_working_status'), name='current_working_status', description=\"created by layer 'current_working_status'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='day_number'), name='day_number', description=\"created by layer 'day_number'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 512), dtype=tf.float32, name='embeddings'), name='embeddings', description=\"created by layer 'embeddings'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='ethnicity'), name='ethnicity', description=\"created by layer 'ethnicity'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='food_cultural_class'), name='food_cultural_class', description=\"created by layer 'food_cultural_class'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='height'), name='height', description=\"created by layer 'height'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='life_style'), name='life_style', description=\"created by layer 'life_style'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='marital_status'), name='marital_status', description=\"created by layer 'marital_status'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='meal_type_y'), name='meal_type_y', description=\"created by layer 'meal_type_y'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='next_BMI'), name='next_BMI', description=\"created by layer 'next_BMI'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='nutrition_goal'), name='nutrition_goal', description=\"created by layer 'nutrition_goal'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='place_of_meal_consumption'), name='place_of_meal_consumption', description=\"created by layer 'place_of_meal_consumption'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='price'), name='price', description=\"created by layer 'price'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='projected_daily_calories'), name='projected_daily_calories', description=\"created by layer 'projected_daily_calories'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='social_situation_of_meal_consumption'), name='social_situation_of_meal_consumption', description=\"created by layer 'social_situation_of_meal_consumption'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='taste'), name='taste', description=\"created by layer 'taste'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='time_of_meal_consumption'), name='time_of_meal_consumption', description=\"created by layer 'time_of_meal_consumption'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='weight'), name='weight', description=\"created by layer 'weight'\")\n"
     ]
    }
   ],
   "source": [
    "model_inputs = model_context_m1.inputs\n",
    "for inpt in model_inputs:\n",
    "  print(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# model inputs\n",
    "tf.keras.utils.plot_model(model_context_m1, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ['nutrition_goal',\n",
    " 'clinical_gender',\n",
    " 'age_range',\n",
    " 'life_style',\n",
    " 'weight',\n",
    " 'height',\n",
    " 'projected_daily_calories',\n",
    " 'current_daily_calories',\n",
    " 'cultural_factor',\n",
    " 'allergy',\n",
    " 'current_working_status',\n",
    " 'marital_status',\n",
    " 'ethnicity',\n",
    " 'BMI',\n",
    " 'next_BMI',\n",
    " 'day_number',\n",
    " 'meal_type_y',\n",
    " 'time_of_meal_consumption',\n",
    " 'place_of_meal_consumption',\n",
    " 'social_situation_of_meal_consumption',\n",
    " 'food_cultural_class',\n",
    " 'calories',\n",
    " 'allergens',\n",
    " 'taste',\n",
    " 'price',\n",
    "  'embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "shape = (np.array(preprossed_df[\"embeddings\"].tolist()).shape[1])\n",
    "print(shape)\n",
    "#full_user_emb = np.vstack(list(dict_users_embeddings.values()))\n",
    "full_recipe_emb = np.vstack(list(dict_embeddings_cbow.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset from pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_from_pandas(df:pd.DataFrame, numeric_features, feature_columns, embedding_columns, target_col):\n",
    "  dict_features = generate_features(df, feature_columns, numeric_features)\n",
    "  dict_embeddings = {}\n",
    "  for emb_col in embedding_columns:\n",
    "    dict_embeddings[emb_col] = tf.convert_to_tensor(np.array(df.loc[:, emb_col].tolist()), dtype=tf.float32)\n",
    "  dict_features.update(dict_embeddings)\n",
    "  #dict_labels = generate_features(train, TARGET_FEATURE_LABELS, TARGET_FEATURE_LABELS)\n",
    "  labels = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(df.loc[:, target_col].values, dtype=tf.float32))\n",
    "  features = tf.data.Dataset.from_tensor_slices(dict_features)\n",
    "  # embeddings_ds = tf.data.Dataset.from_tensor_slices(dict_features)\n",
    "  # features_ds = tf.data.Dataset.zip((features, embeddings_ds))\n",
    "  ds = tf.data.Dataset.zip((features, labels))\n",
    "  #train_ds = tf.data.Dataset.from_tensor_slices((features_list, labels_list))\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features_without_emb = ['BMI',\n",
    " 'age_range',\n",
    " 'allergens',\n",
    " 'allergy',\n",
    " 'calories',\n",
    " 'clinical_gender',\n",
    " 'cultural_factor',\n",
    " 'current_daily_calories',\n",
    " 'current_working_status',\n",
    " 'day_number',\n",
    " 'ethnicity',\n",
    " 'food_cultural_class',\n",
    " 'height',\n",
    " 'life_style',\n",
    " 'marital_status',\n",
    " 'meal_type_y',\n",
    " 'next_BMI',\n",
    " 'nutrition_goal',\n",
    " 'place_of_meal_consumption',\n",
    " 'price',\n",
    " 'projected_daily_calories',\n",
    " 'social_situation_of_meal_consumption',\n",
    " 'taste',\n",
    " 'time_of_meal_consumption',\n",
    " 'weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_float(text: str):\n",
    "  value = re.findall(r'\\d', text)\n",
    "  if len(value) == 0:\n",
    "    return 2.0\n",
    "  else:\n",
    "    return float(value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3356214/1025819399.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  preprossed_df['price'] =preprossed_df['price'].apply(lambda x: transform_to_float(x))\n",
      "/tmp/ipykernel_3356214/1025819399.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  preprossed_df_val['price'] = preprossed_df_val['price'].apply(lambda x: transform_to_float(x))\n",
      "/tmp/ipykernel_3356214/1025819399.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  preprossed_df_test['price'] = preprossed_df_test['price'].apply(lambda x: transform_to_float(x))\n"
     ]
    }
   ],
   "source": [
    "preprossed_df['price'] =preprossed_df['price'].apply(lambda x: transform_to_float(x))\n",
    "preprossed_df_val['price'] = preprossed_df_val['price'].apply(lambda x: transform_to_float(x))\n",
    "preprossed_df_test['price'] = preprossed_df_test['price'].apply(lambda x: transform_to_float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 17:15:25.279099: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1217290240 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_ds = generate_dataset_from_pandas(preprossed_df.sample(frac=1),\n",
    "                                        numeric_features,\n",
    "                                        feature_columns=input_features_without_emb,\n",
    "                                        embedding_columns=[\"embeddings\"],\n",
    "                                        target_col=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 17:15:31.064887: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 320282624 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "val_ds = generate_dataset_from_pandas(preprossed_df_val.sample(frac=1),\n",
    "                                      numeric_features,\n",
    "                                      feature_columns=input_features_without_emb,\n",
    "                                      embedding_columns=[\"embeddings\"],\n",
    "                                      target_col=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 17:15:34.721664: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 419956736 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "test_ds = generate_dataset_from_pandas(preprossed_df_test,\n",
    "                                      numeric_features,\n",
    "                                      feature_columns=input_features_without_emb,\n",
    "                                      embedding_columns=[\"embeddings\"],\n",
    "                                      target_col=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check embeddings and batch\n",
    "train_ds = train_ds.batch(batch_size=128).shuffle(1024)\n",
    "val_ds = val_ds.batch(batch_size=128).shuffle(1024)\n",
    "test_ds = test_ds.batch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 17:15:41.368553: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 419956736 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1603/1603 [==============================] - 8s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "y_test_pred_raw = model_context_m1.predict(test_ds)\n",
    "y_test_pred = np.rint(y_test_pred_raw)\n",
    "y_true = np.concatenate([y for x, y in test_ds], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.99    102919\n",
      "         1.0       0.98      0.99      0.99    102138\n",
      "\n",
      "    accuracy                           0.99    205057\n",
      "   macro avg       0.99      0.99      0.99    205057\n",
      "weighted avg       0.99      0.99      0.99    205057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9846833751207281\n",
      "Recall: 0.9882022361902524\n"
     ]
    }
   ],
   "source": [
    "precision_test = precision_score(y_true, y_test_pred)\n",
    "recall_test = recall_score(y_true, y_test_pred)\n",
    "\n",
    "print(f\"Precision: {precision_test}\")\n",
    "print(f\"Recall: {recall_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cnf = confusion_matrix(y_true, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cnf, display_labels=[\"dislike\", \"like\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fe936535730>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract rules from the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 17:17:13.276528: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1217290240 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_ds = generate_dataset_from_pandas(preprossed_df.sample(frac=1),\n",
    "                                        numeric_features,\n",
    "                                        feature_columns=input_features_without_emb,\n",
    "                                        embedding_columns=[\"embeddings\"],\n",
    "                                        target_col=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [f\"emb_{i}\" for i in range(0, 700)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4644/4644 [==============================] - 23s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model_context_m1.predict(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dexire.dexire import DEXiRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dexire = DEXiRE(model=model_context_m1,\n",
    "                feature_names=feature_names,\n",
    "                class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/4644 [..............................] - ETA: 1:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4644/4644 [==============================] - 22s 5ms/step\n",
      "Number of classes: [0. 1.]\n",
      "4644/4644 [==============================] - 20s 4ms/step\n",
      "318\n",
      "emb_318\n",
      "318\n",
      "emb_318\n",
      "318\n",
      "emb_318\n",
      "318\n",
      "emb_318\n",
      "185\n",
      "emb_185\n",
      "318\n",
      "emb_318\n",
      "182\n",
      "emb_182\n",
      "497\n",
      "emb_497\n",
      "396\n",
      "emb_396\n",
      "396\n",
      "emb_396\n",
      "396\n",
      "emb_396\n"
     ]
    }
   ],
   "source": [
    "rules = dexire.extract_rules(train_ds, layer_idx=-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((emb_318 > -0.191) AND (emb_396 <= 0.183) AND (emb_396 <= 0.152) AND (emb_396 <= 0.125))\n",
      "((emb_318 <= -0.191) AND (emb_318 <= -0.235) AND (emb_318 <= -0.271) AND (emb_318 <= -0.297) AND (emb_185 > -5.722) AND (emb_318 <= -0.325) AND (emb_182 <= 24.713) AND (emb_497 > -1.245))\n",
      "((emb_318 <= -0.191) AND (emb_318 > -0.235))\n",
      "((emb_318 <= -0.191) AND (emb_318 <= -0.235) AND (emb_318 <= -0.271) AND (emb_318 <= -0.297) AND (emb_185 > -5.722) AND (emb_318 <= -0.325) AND (emb_182 <= 24.713) AND (emb_497 <= -1.245))\n",
      "((emb_318 <= -0.191) AND (emb_318 <= -0.235) AND (emb_318 > -0.271))\n",
      "((emb_318 <= -0.191) AND (emb_318 <= -0.235) AND (emb_318 <= -0.271) AND (emb_318 <= -0.297) AND (emb_185 > -5.722) AND (emb_318 > -0.325))\n",
      "((emb_318 > -0.191) AND (emb_396 <= 0.183) AND (emb_396 <= 0.152) AND (emb_396 > 0.125))\n",
      "((emb_318 <= -0.191) AND (emb_318 <= -0.235) AND (emb_318 <= -0.271) AND (emb_318 > -0.297))\n",
      "((emb_318 > -0.191) AND (emb_396 <= 0.183) AND (emb_396 > 0.152))\n",
      "((emb_318 <= -0.191) AND (emb_318 <= -0.235) AND (emb_318 <= -0.271) AND (emb_318 <= -0.297) AND (emb_185 > -5.722) AND (emb_318 <= -0.325) AND (emb_182 > 24.713))\n",
      "((emb_318 <= -0.191) AND (emb_318 <= -0.235) AND (emb_318 <= -0.271) AND (emb_318 <= -0.297) AND (emb_185 <= -5.722))\n",
      "((emb_318 > -0.191) AND (emb_396 > 0.183))\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "# count total terms\n",
    "total = 0\n",
    "for rule in rules.rules:\n",
    "  print(rule.premise)\n",
    "  total += len(rule.premise.clauses)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rules \n",
    "with open('rules_use.pkl', 'bw') as f:\n",
    "    pickle.dump(rules, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intermediate_model(full_model, layer_idx):\n",
    "  intermediate_layer_model = tf.keras.models.Model(inputs=full_model.input,\n",
    "                                 outputs=full_model.layers[layer_idx].output)\n",
    "  return intermediate_layer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_model = get_intermediate_model(model_context_m1, -6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1603/1603 [==============================] - 7s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "x_test = partial_model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = np.random.choice(len(x_test), 1000)\n",
    "x_test = x_test[rows]\n",
    "y_true = y_true[rows]\n",
    "y_test_pred = y_test_pred[rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rs = rules.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rs_np = np.zeros_like(y_true)\n",
    "for i in range(len(y_rs_np)):\n",
    "  if y_rs[i][0][-1] == \"1\":\n",
    "    y_rs_np[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['class: 0', 'class: 1'], dtype='<U8')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.984\n",
      "Fiderlity: 0.995\n",
      "Precision: 0.9806201550387597\n",
      "Recall: 0.98828125\n",
      "F1: 0.9844357976653697\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_true, y_rs_np)\n",
    "fiderlity = accuracy_score(y_test_pred, y_rs_np)\n",
    "precision = precision_score(y_true, y_rs_np)\n",
    "recall = recall_score(y_true, y_rs_np)\n",
    "f1 = f1_score(y_true, y_rs_np)\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Fiderlity: {fiderlity}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_rs_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(features_df, model_inputs, ebedding_cols=[]):\n",
    "  dict_features = {}\n",
    "  for inp in model_inputs:\n",
    "    # Embedding codifying\n",
    "    if inp.name in ebedding_cols:\n",
    "      dict_features[inp.name] = tf.convert_to_tensor(np.stack(features_df[inp.name].values),\n",
    "                                                     dtype=tf.float32)\n",
    "    elif inp.dtype.name == 'string':\n",
    "      dict_features[inp.name] = tf.convert_to_tensor(features_df[inp.name].values, dtype=tf.string)\n",
    "    elif inp.dtype.name == 'float32':\n",
    "      dict_features[inp.name] = tf.convert_to_tensor(features_df[inp.name].values, dtype=tf.float32)\n",
    "    elif inp.dtype.name == 'float64':\n",
    "      dict_features[inp.name] = tf.convert_to_tensor(features_df[inp.name].values, dtype=tf.float64)\n",
    "    elif inp.dtype.name == 'int64':\n",
    "      dict_features[inp.name] = tf.convert_to_tensor(features_df[inp.name].values, dtype=tf.int64)\n",
    "    else:\n",
    "      raise Exception(\"No recognized data type\")\n",
    "  return dict_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nutrition_goal', 'clinical_gender', 'age_range', 'life_style',\n",
       "       'weight', 'height', 'projected_daily_calories',\n",
       "       'current_daily_calories', 'cultural_factor', 'allergy',\n",
       "       'current_working_status', 'marital_status', 'ethnicity', 'BMI',\n",
       "       'next_BMI', 'day_number', 'meal_type_y', 'time_of_meal_consumption',\n",
       "       'place_of_meal_consumption', 'social_situation_of_meal_consumption',\n",
       "       'food_cultural_class', 'calories', 'allergens', 'taste', 'price',\n",
       "       'recipeId', 'label', 'embeddings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprossed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_bn = user_based_filtered_test.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_bn['embeddings'] = [dict_embeddings_cbow[i] for i in data_for_bn['recipeId'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique users\n",
    "users_id = data_for_bn['userId'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/anaconda3/envs/pro_dexire/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator KMeans from version 1.2.2 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load cluster\n",
    "cluster_path = os.path.join(base_path, 'data', 'k_means_use.pkl')\n",
    "with open(cluster_path, 'rb') as f:\n",
    "  kmeans = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_bn['recipe_cluster'] = kmeans.predict(np.vstack(data_for_bn['embeddings'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['day_number', 'meal_type_x', 'userId', 'foodId',\n",
       "       'time_of_meal_consumption', 'place_of_meal_consumption',\n",
       "       'social_situation_of_meal_consumption', 'nutrition_goal',\n",
       "       'clinical_gender', 'age_range', 'life_style', 'weight', 'height',\n",
       "       'projected_daily_calories', 'current_daily_calories',\n",
       "       'country_of_origin', 'living_country', 'current_location',\n",
       "       'cultural_factor', 'probabilities', 'allergy', 'Multi-allergy',\n",
       "       'current_working_status', 'marital_status', 'ethnicity', 'BMI',\n",
       "       'next_BMI', 'recipe_name', 'recipe_raw_text', 'meal_type_y',\n",
       "       'food_cultural_class', 'recipeId', 'label__Appreciated',\n",
       "       'label__Avoidable', 'label__Favorable', 'label__Neutral',\n",
       "       'label__Never', 'label', 'calories', 'allergens', 'taste', 'price',\n",
       "       'embeddings', 'recipe_cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_bn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(x):\n",
    "  l = re.findall(r'\\d+\\.\\d+', x)\n",
    "  if len(l) == 0:\n",
    "    return 2.0\n",
    "  result = float(l[0])\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_bn['price'] = data_for_bn['price'].apply(lambda x: price(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = prepare_input(data_for_bn, model_context_m1.inputs, ebedding_cols=['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_labels = model_context_m1.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bin = np.rint(y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_bn['y_pred'] = y_pred_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_for_bn.to_csv(\"data_for_bn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchical_edges(parents_list: List[str], child_node: str):\n",
    "  list_of_edges = []\n",
    "  for parent in parents_list:\n",
    "    list_of_edges.append((parent, child_node))\n",
    "  return list_of_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features =  ['nutrition_goal', 'clinical_gender', 'age_range', 'life_style',\n",
    "                      'cultural_factor', 'allergy', 'BMI',  'meal_type_y',\n",
    "       'place_of_meal_consumption', 'social_situation_of_meal_consumption',\n",
    "       'food_cultural_class', 'calories', 'allergens', 'taste', 'price',\n",
    "                 'recipe_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = create_hierarchical_edges(relevant_features,  'y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.4\n",
      "0.1.25\n",
      "0.8.7\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "print(matplotlib.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Version pgmpy should be >= 0.1.13\n",
    "import pgmpy\n",
    "print(pgmpy.__version__)\n",
    "\n",
    "# Latest version bnlearn\n",
    "import bnlearn as bn\n",
    "print(bn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_consider = ['day_number', 'time_of_meal_consumption', \n",
    "                       'place_of_meal_consumption',\n",
    "                       'social_situation_of_meal_consumption', \n",
    "                       'nutrition_goal',\n",
    "                       'clinical_gender', \n",
    "                       'age_range', \n",
    "                       'life_style', \n",
    "                       'weight', \n",
    "                       'height',\n",
    "                       'projected_daily_calories', \n",
    "                       'current_daily_calories',\n",
    "                       'cultural_factor', \n",
    "                       'allergy',\n",
    "                       'current_working_status', \n",
    "                       'marital_status', \n",
    "                       'ethnicity', \n",
    "                       'BMI',\n",
    "                       'next_BMI', \n",
    "                       'food_cultural_class',  \n",
    "                       'calories', \n",
    "                       'allergens', \n",
    "                       'taste', \n",
    "                       'price',\n",
    "                       'recipe_cluster', \n",
    "                       'y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_bn = data_for_bn[columns_to_consider]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Computing best DAG using [hc]\n",
      "[bnlearn] >Set scoring type at [bic]\n",
      "[bnlearn] >Compute structure scores for model comparison (higher is better).\n"
     ]
    }
   ],
   "source": [
    "# Structure learning\n",
    "model = bn.structure_learning.fit(data_for_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Parameter learning> Computing parameters using [bayes]\n",
      "[bnlearn] >Converting [<class 'pgmpy.base.DAG.DAG'>] to BayesianNetwork model.\n",
      "[bnlearn] >Converting adjmat to BayesianNetwork.\n",
      "[bnlearn] >CPD of allergy:\n",
      "+-------------------------+--------+\n",
      "| allergy(Multiple)       | 0.073  |\n",
      "+-------------------------+--------+\n",
      "| allergy(NotRestriction) | 0.0905 |\n",
      "+-------------------------+--------+\n",
      "| allergy(cow's milk)     | 0.109  |\n",
      "+-------------------------+--------+\n",
      "| allergy(eggs)           | 0.0925 |\n",
      "+-------------------------+--------+\n",
      "| allergy(fish)           | 0.122  |\n",
      "+-------------------------+--------+\n",
      "| allergy(peanut)         | 0.1075 |\n",
      "+-------------------------+--------+\n",
      "| allergy(shellfish)      | 0.116  |\n",
      "+-------------------------+--------+\n",
      "| allergy(soy)            | 0.0945 |\n",
      "+-------------------------+--------+\n",
      "| allergy(tree nuts)      | 0.0985 |\n",
      "+-------------------------+--------+\n",
      "| allergy(wheat)          | 0.0965 |\n",
      "+-------------------------+--------+\n",
      "[bnlearn] >CPD of food_cultural_class:\n",
      "+-----+---------------------+\n",
      "| ... | allergy(wheat)      |\n",
      "+-----+---------------------+\n",
      "| ... | 0.12953367875647667 |\n",
      "+-----+---------------------+\n",
      "| ... | 0.17098445595854922 |\n",
      "+-----+---------------------+\n",
      "| ... | 0.17098445595854922 |\n",
      "+-----+---------------------+\n",
      "| ... | 0.33678756476683935 |\n",
      "+-----+---------------------+\n",
      "| ... | 0.19170984455958548 |\n",
      "+-----+---------------------+\n",
      "[bnlearn] >CPD of BMI:\n",
      "+------------------+--------+\n",
      "| BMI(healthy)     | 0.2895 |\n",
      "+------------------+--------+\n",
      "| BMI(obesity)     | 0.2285 |\n",
      "+------------------+--------+\n",
      "| BMI(overweight)  | 0.3115 |\n",
      "+------------------+--------+\n",
      "| BMI(underweight) | 0.1705 |\n",
      "+------------------+--------+\n",
      "[bnlearn] >CPD of nutrition_goal:\n",
      "+------------------------------+-----+--------------------+\n",
      "| BMI                          | ... | BMI(underweight)   |\n",
      "+------------------------------+-----+--------------------+\n",
      "| nutrition_goal(gain_weight)  | ... | 0.5112414467253177 |\n",
      "+------------------------------+-----+--------------------+\n",
      "| nutrition_goal(lose_weight)  | ... | 0.2443792766373412 |\n",
      "+------------------------------+-----+--------------------+\n",
      "| nutrition_goal(maintain_fit) | ... | 0.2443792766373412 |\n",
      "+------------------------------+-----+--------------------+\n",
      "[bnlearn] >CPD of next_BMI:\n",
      "+-----------------------+-----+---------------------+\n",
      "| BMI                   | ... | BMI(underweight)    |\n",
      "+-----------------------+-----+---------------------+\n",
      "| next_BMI(healthy)     | ... | 0.28005865102639294 |\n",
      "+-----------------------+-----+---------------------+\n",
      "| next_BMI(obesity)     | ... | 0.18328445747800587 |\n",
      "+-----------------------+-----+---------------------+\n",
      "| next_BMI(overweight)  | ... | 0.18328445747800587 |\n",
      "+-----------------------+-----+---------------------+\n",
      "| next_BMI(underweight) | ... | 0.3533724340175953  |\n",
      "+-----------------------+-----+---------------------+\n",
      "[bnlearn] >CPD of recipe_cluster:\n",
      "+---------------------+-----+---------------------------------+\n",
      "| food_cultural_class | ... | food_cultural_class(vegetarian) |\n",
      "+---------------------+-----+---------------------------------+\n",
      "| recipe_cluster(0)   | ... | 0.28846153846153844             |\n",
      "+---------------------+-----+---------------------------------+\n",
      "| recipe_cluster(1)   | ... | 0.23076923076923078             |\n",
      "+---------------------+-----+---------------------------------+\n",
      "| recipe_cluster(2)   | ... | 0.2153846153846154              |\n",
      "+---------------------+-----+---------------------------------+\n",
      "| recipe_cluster(3)   | ... | 0.2653846153846154              |\n",
      "+---------------------+-----+---------------------------------+\n",
      "[bnlearn] >CPD of y_pred:\n",
      "+---------------------+-----+---------------------------------+\n",
      "| food_cultural_class | ... | food_cultural_class(vegetarian) |\n",
      "+---------------------+-----+---------------------------------+\n",
      "| y_pred(0.0)         | ... | 0.6153846153846154              |\n",
      "+---------------------+-----+---------------------------------+\n",
      "| y_pred(1.0)         | ... | 0.38461538461538464             |\n",
      "+---------------------+-----+---------------------------------+\n",
      "[bnlearn] >CPD of taste:\n",
      "+-----+----------------------+\n",
      "| ... | recipe_cluster(3)    |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.03503267973856209  |\n",
      "+-----+----------------------+\n",
      "| ... | 0.03503267973856209  |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.060915032679738565 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.3950326797385621   |\n",
      "+-----+----------------------+\n",
      "| ... | 0.049150326797385624 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.032679738562091505 |\n",
      "+-----+----------------------+\n",
      "[bnlearn] >CPD of price:\n",
      "+----------------+-----+---------------------+\n",
      "| recipe_cluster | ... | recipe_cluster(3)   |\n",
      "+----------------+-----+---------------------+\n",
      "| price(1.0)     | ... | 0.25529411764705884 |\n",
      "+----------------+-----+---------------------+\n",
      "| price(2.0)     | ... | 0.44588235294117645 |\n",
      "+----------------+-----+---------------------+\n",
      "| price(2.5)     | ... | 0.14705882352941177 |\n",
      "+----------------+-----+---------------------+\n",
      "| price(3.0)     | ... | 0.15176470588235294 |\n",
      "+----------------+-----+---------------------+\n",
      "[bnlearn] >CPD of place_of_meal_consumption:\n",
      "+-----+---------------------+\n",
      "| ... | y_pred(1.0)         |\n",
      "+-----+---------------------+\n",
      "| ... | 0.403765011359948   |\n",
      "+-----+---------------------+\n",
      "| ... | 0.27913015254787404 |\n",
      "+-----+---------------------+\n",
      "| ... | 0.3171048360921778  |\n",
      "+-----+---------------------+\n",
      "[bnlearn] >CPD of calories:\n",
      "+-------------------+----------------------+----------------------+\n",
      "| y_pred            | y_pred(0.0)          | y_pred(1.0)          |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(60.0)    | 0.007137147424917205 | 0.013577842691766734 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(70.0)    | 0.007137147424917205 | 0.012604132857297405 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(80.0)    | 0.007137147424917205 | 0.012604132857297405 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(90.0)    | 0.008164896654105282 | 0.01844639186411337  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(100.0)   | 0.00919264588329336  | 0.028183490208806644 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(120.0)   | 0.015359141258421824 | 0.01065671318835875  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(130.0)   | 0.007137147424917205 | 0.011630423022828079 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(150.0)   | 0.030775379696242982 | 0.012604132857297405 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(160.0)   | 0.011248144341669514 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(170.0)   | 0.00919264588329336  | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(180.0)   | 0.023581135091926442 | 0.019420101698582697 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(187.0)   | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(188.0)   | 0.007137147424917205 | 0.01747268202964404  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(190.0)   | 0.00919264588329336  | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(200.0)   | 0.03796962430055952  | 0.012604132857297405 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(210.0)   | 0.00919264588329336  | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(220.0)   | 0.013303642800045668 | 0.027209780374337317 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(230.0)   | 0.00919264588329336  | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(234.0)   | 0.007137147424917205 | 0.011630423022828079 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(240.0)   | 0.014331392029233746 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(245.0)   | 0.007137147424917205 | 0.02526236070539866  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(249.0)   | 0.00919264588329336  | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(250.0)   | 0.04002512275893568  | 0.019420101698582697 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(260.0)   | 0.011248144341669514 | 0.020393811533052024 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(270.0)   | 0.007137147424917205 | 0.013577842691766734 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(280.0)   | 0.011248144341669514 | 0.01844639186411337  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(293.0)   | 0.011248144341669514 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(296.0)   | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(300.0)   | 0.0389973735297476   | 0.03792058855349992  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(301.0)   | 0.007137147424917205 | 0.015525262360705389 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(312.0)   | 0.007137147424917205 | 0.019420101698582697 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(320.0)   | 0.011248144341669514 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(332.0)   | 0.007137147424917205 | 0.01844639186411337  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(346.0)   | 0.007137147424917205 | 0.012604132857297405 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(350.0)   | 0.08216284115564686  | 0.04668397706372387  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(360.0)   | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(370.0)   | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(380.0)   | 0.021525636633550286 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(383.0)   | 0.007137147424917205 | 0.016498972195174714 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(390.0)   | 0.007137147424917205 | 0.016498972195174714 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(400.0)   | 0.0801073426972707   | 0.042789137725846554 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(410.0)   | 0.007137147424917205 | 0.011630423022828079 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(420.0)   | 0.012275893570857592 | 0.01747268202964404  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(425.0)   | 0.007137147424917205 | 0.02136752136752135  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(440.0)   | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(450.0)   | 0.046191618134064145 | 0.01747268202964404  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(461.0)   | 0.007137147424917205 | 0.02234123120199068  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(480.0)   | 0.013303642800045668 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(500.0)   | 0.03385862738380722  | 0.019420101698582697 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(549.0)   | 0.007137147424917205 | 0.015525262360705389 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(550.0)   | 0.0163868904876099   | 0.01844639186411337  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(600.0)   | 0.022553385862738362 | 0.019420101698582697 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(650.0)   | 0.007137147424917205 | 0.019420101698582697 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(680.0)   | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(700.0)   | 0.013303642800045668 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(720.0)   | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(750.0)   | 0.007137147424917205 | 0.015525262360705389 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(800.0)   | 0.010220395112481436 | 0.02915720004327597  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(900.0)   | 0.007137147424917205 | 0.01844639186411337  |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(920.0)   | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(1000.0)  | 0.008164896654105282 | 0.012604132857297405 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(1335.0)  | 0.00919264588329336  | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(1400.0)  | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(1904.0)  | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(2000.0)  | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(2052.0)  | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(2570.0)  | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(2608.0)  | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(3283.0)  | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(4522.0)  | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(7973.0)  | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "| calories(10776.0) | 0.008164896654105282 | 0.006761873850481441 |\n",
      "+-------------------+----------------------+----------------------+\n",
      "[bnlearn] >CPD of allergens:\n",
      "+-----+----------------------+\n",
      "| ... | y_pred(1.0)          |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.40766591742576497  |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.017208273803565374 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.010392304962280094 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.021103113141442677 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.009418595127810769 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.009418595127810769 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.005523755789933465 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.015260854134626724 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.01623456396909605  |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.013313434465688072 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.01623456396909605  |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.024024242644850657 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.006497465624402791 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "| ... | 0.004550045955464139 |\n",
      "+-----+----------------------+\n",
      "[bnlearn] >Compute structure scores for model comparison (higher is better).\n",
      "[bnlearn] >WARNING> Skipping computing structure score for [k2].\n"
     ]
    }
   ],
   "source": [
    "# Parameter learning\n",
    "model = bn.parameter_learning.fit(model, data_for_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pypickle] Pickle file saved: [/home/victor/Documents/pro_dexire/models/bnlearn_model.pkl]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.save(model, filepath=os.path.join(base_path, 'models', 'bnlearn_model'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['day_number', 'time_of_meal_consumption', 'place_of_meal_consumption',\n",
       "       'social_situation_of_meal_consumption', 'nutrition_goal',\n",
       "       'clinical_gender', 'age_range', 'life_style', 'weight', 'height',\n",
       "       'projected_daily_calories', 'current_daily_calories', 'cultural_factor',\n",
       "       'allergy', 'current_working_status', 'marital_status', 'ethnicity',\n",
       "       'BMI', 'next_BMI', 'food_cultural_class', 'calories', 'allergens',\n",
       "       'taste', 'price', 'recipe_cluster', 'y_pred'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_bn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Variable Elimination.\n",
      "[bnlearn] >Warning: variable(s) [None] does not exists in DAG.\n",
      "[bnlearn] >Data is stored in [query.df]\n",
      "+----+----------+---------+\n",
      "|    |   y_pred |       p |\n",
      "+====+==========+=========+\n",
      "|  0 |        0 | 0.56082 |\n",
      "+----+----------+---------+\n",
      "|  1 |        1 | 0.43918 |\n",
      "+----+----------+---------+\n"
     ]
    }
   ],
   "source": [
    "query = bn.inference.fit(model, variables=['y_pred'], evidence={'price':3, \n",
    "                                                                'recipe_cluster': 0}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[d3blocks] >INFO> Cleaning edge_properties and config parameters..\n",
      "[d3blocks] >INFO> Converting source-target into adjacency matrix..\n",
      "[d3blocks] >INFO> Making the matrix symmetric..\n",
      "[d3blocks] >INFO> Set directed=True to see the markers!\n",
      "[d3blocks] >INFO> Keep only edges with weight>0\n",
      "[d3blocks] >INFO> Converting source-target into adjacency matrix..\n",
      "[d3blocks] >INFO> Making the matrix symmetric..\n",
      "[d3blocks] >INFO> Converting adjacency matrix into source-target..\n",
      "[d3blocks] >INFO> Number of unique nodes: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[d3blocks] >INFO> Slider range is set to [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Set node properties.\n",
      "[bnlearn] >Set edge properties.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[d3blocks] >INFO> Write to path: [/tmp/tmp5950o4l9/d3graph.html]\n",
      "[d3blocks] >INFO> File already exists and will be overwritten: [/tmp/tmp5950o4l9/d3graph.html]\n",
      "[d3blocks] >INFO> Keep only edges with weight>0\n",
      "[d3blocks] >INFO> Converting source-target into adjacency matrix..\n",
      "[d3blocks] >INFO> Making the matrix symmetric..\n",
      "[d3blocks] >INFO> Number of unique nodes: 12\n",
      "[d3blocks] >INFO> Slider range is set to [0, 1]\n",
      "[d3blocks] >INFO> Write to path: [/tmp/tmpljd01juo/bnlearn_Directed_Acyclic_Graph_(DAG).html]\n",
      "[d3blocks] >INFO> File already exists and will be overwritten: [/tmp/tmpljd01juo/bnlearn_Directed_Acyclic_Graph_(DAG).html]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fig': '/tmp/tmpljd01juo/bnlearn_Directed_Acyclic_Graph_(DAG).html',\n",
       " 'ax': '/tmp/tmpljd01juo/bnlearn_Directed_Acyclic_Graph_(DAG).html',\n",
       " 'pos': None,\n",
       " 'G': <networkx.classes.digraph.DiGraph at 0x7fe93ef5b280>,\n",
       " 'node_properties': {'day_number': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'time_of_meal_consumption': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'place_of_meal_consumption': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'social_situation_of_meal_consumption': {'node_color': '#1f456e',\n",
       "   'node_size': 10},\n",
       "  'nutrition_goal': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'clinical_gender': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'age_range': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'life_style': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'weight': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'height': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'projected_daily_calories': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'current_daily_calories': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'cultural_factor': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'allergy': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'current_working_status': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'marital_status': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'ethnicity': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'BMI': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'next_BMI': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'food_cultural_class': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'calories': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'allergens': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'taste': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'price': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'recipe_cluster': {'node_color': '#1f456e', 'node_size': 10},\n",
       "  'y_pred': {'node_color': '#1f456e', 'node_size': 10}},\n",
       " 'edge_properties': {('allergy', 'food_cultural_class'): {'color': '#000000',\n",
       "   'weight': 1},\n",
       "  ('BMI', 'nutrition_goal'): {'color': '#000000', 'weight': 1},\n",
       "  ('BMI', 'next_BMI'): {'color': '#000000', 'weight': 1},\n",
       "  ('food_cultural_class', 'recipe_cluster'): {'color': '#000000', 'weight': 1},\n",
       "  ('food_cultural_class', 'y_pred'): {'color': '#000000', 'weight': 1},\n",
       "  ('recipe_cluster', 'taste'): {'color': '#000000', 'weight': 1},\n",
       "  ('recipe_cluster', 'price'): {'color': '#000000', 'weight': 1},\n",
       "  ('y_pred', 'place_of_meal_consumption'): {'color': '#000000', 'weight': 1},\n",
       "  ('y_pred', 'calories'): {'color': '#000000', 'weight': 1},\n",
       "  ('y_pred', 'allergens'): {'color': '#000000', 'weight': 1}}}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.plot(model, \n",
    "        interactive=True, \n",
    "        params_interactive = {'height':'800px', 'width':'70%', 'layout':None, 'bgcolor':'#0f0f0f0f'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_2_15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
